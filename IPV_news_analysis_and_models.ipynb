{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using NLP to Classify News Articles about Intimate Partner Violence\n",
    "\n",
    "### Capstone project by Sean Justice for the Fall 2018 cohort at the New York City Data Science Academy\n",
    "Further details available on [this blog post](https://nycdatascience.com/blog/student-works/using-nlp-to-classify-articles-about-intimate-partner-violence/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dataset generated from parsing the [pdf on Arizona intimate partner violence](https://everytownresearch.org/documents/2015/09/census-domestic-violence-gun-homicides-arizona.pdf)\n",
    "- PDF contains information on 105 instances of IPV in Arizona from 2009 to 2013\n",
    "    - Research performed by [Everytown for Gun Safety](https://everytownresearch.org/)\n",
    "- Each entry contains the data of the incident, city where it occurred, and a paragraph summarizing the incident\n",
    "- Also contained are some labels that classify the type of incident\n",
    "     - Was this a shooter suicide incident?\n",
    "     - Was there a history of domestic violence?\n",
    "     - Were they any prior convictions in the case?\n",
    "     - Had an order of protection been granted by the courts?\n",
    "     - Did the order require firearms be turned in to law inforcement?\n",
    "     - Did the federal government prohibit the attacker from possessing firearms?\n",
    "- Just two of the subclassifications will be used for creating a model using news articles about the incidents\n",
    "    - Shooter suicide\n",
    "    - History of domestic violence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df = pd.read_csv('./data/census-domestic-violence-arizona.csv', skiprows=[1], na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform named entity extraction to get the names of all those involved in each instance of IPV\n",
    "- These names will be used for finding news articles about each instance\n",
    "- Using both nltk and spacy for named entity extraction since I found that on their own they missed some names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK based named entity extraction\n",
    "def extract_entities(text):\n",
    "    ret_list = []\n",
    "    '''\n",
    "    Divide each text portion of the incident in to sentences, tokenize each\n",
    "    sentence, and then apply the part of speech tag to it, and combine those in \n",
    "    to chunks. Return only the chunks that are labeled as a PERSON\n",
    "    '''\n",
    "    #  Divide each text portion of the incident in to sentences, tokenize each\n",
    "    # sentence, and then apply the part of speech tag to it, and combine those in \n",
    "    # to chunks. Once those chunks are labeled, filter to return only the ones that\n",
    "    # are labeled as a PERSON\n",
    "    \n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                cur_name = ' '.join([c[0] for c in chunk.leaves()])\n",
    "                if cur_name not in ret_list:\n",
    "                    ret_list.append(cur_name)\n",
    "    return(ret_list)\n",
    "\n",
    "az_df['ne_names'] = az_df.Text.apply(extract_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small english model for spacy to perform named entity detection\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def parse_nlp_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    # Return the items in the text that are labeled as PERSON\n",
    "    return [X.text for X in doc.ents if X.label_ == 'PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_df['spacy_names'] = az_df.Text.apply(parse_nlp_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unravel the lists in the ne_names and spacy_names columns and put them in a set then join the union of the sets\n",
    "az_df['names'] = az_df.apply(lambda x: ','.join({*x.ne_names} | {*x.spacy_names}), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_ipv_data = az_df[['Text', 'Location', 'Date', 'shooter_suicide', 'dv_history', 'names']].copy()\n",
    "az_ipv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate writing of data to csv\n",
    "# az_ipv_data.to_csv('./az_ipv_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use bing search to find news articles about each incident using the names\n",
    "- Uses Microsoft's Azure Cognitive Search service\n",
    "- Each search returns ten results\n",
    "- Search uses two keys that are read from the ./data/bing_keys.json config file\n",
    "- Tutorial on how to setup Azure search can be found here: https://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/quick-start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/bing_keys.json') as f:\n",
    "    data = json.load(f)\n",
    "bing_key1 = data['bing_key1']\n",
    "bing_key2 = data['bing_key2']\n",
    "bing_search_url = 'https://api.cognitive.microsoft.com/bing/v7.0/search'\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites that appear in the results but do not give meaningful results\n",
    "website_ban = ['spokeo', 'facebook', 'twitter', 'ancestry', 'youtube', 'tripadvisor', 'gofundme', 'mylife', 'amazon',\n",
    "               'peoplesmart', 'beenverified', 'myheritage', 'obituary', 'obituaries', 'whitepages', 'findagrave',\n",
    "               'peoplefinder', 'peekyou', 'courtlistener', 'elder-law', 'linkedin', 'imdb', 'wikipedia', 'celebrity',\n",
    "               'mugshots', 'walsh', 'madehoops', 'trakt.tv', 'tvmaze', 'brianshealinghearts', 'medicine.yale', 'freerepublic',\n",
    "               'nocera.blogs', 'webuygoldandsilver', 'aminoapps', 'contempglass', 'earnthenecklace', 'change.org', 'pdf', 'xlsx']\n",
    "\n",
    "# The search query is the proper names of each individual in the incident\n",
    "# Each query returns a json that \n",
    "def bing_search(text):\n",
    "    #print(text)\n",
    "    text = ','.join([x for x in text.split(',') if x.count(' ') > 0])\n",
    "    url_list = []\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\" : bing_key1, 'UserAgent':user_agent, 'Pragma': 'no-cache'}\n",
    "    params  = {\"q\":text, \"textDecorations\":True, \"textFormat\":\"HTML\"}\n",
    "    response = requests.get(bing_search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    response_json = response.json()\n",
    "    # Use try to handle when no results are returned for a query\n",
    "    # Also don't include any results that are in the list of banned websites\n",
    "    # Result is a list of urls for each incident\n",
    "    try:    \n",
    "        for inst in response_json['webPages']['value']:\n",
    "            if not any(x in inst['url'] for x in website_ban):\n",
    "                url_list.append(inst['url'])\n",
    "        if not url_list:\n",
    "            print('No results for search term: {}'.format(text))\n",
    "        return url_list\n",
    "    except KeyError:\n",
    "        print('No results for search term: {}'.format(text))\n",
    "        return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_ipv_data['url_list'] = az_ipv_data.names.apply(bing_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that don't have any urls associated with them\n",
    "az_ipv_data = az_ipv_data.loc[~az_ipv_data.url_list.apply(lambda x: len(x) > 0)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_ipv_data = pd.read_csv('./data/az_ipv_data_with_urls2.csv', index_col=0)\n",
    "az_ipv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe where each url has a row\n",
    "az_ipv_url_df = pd.DataFrame(az_ipv_data.url_list.apply(lambda x: ' '.join(x)).str.split(expand=True).stack())\n",
    "az_ipv_url_df.index = az_ipv_url_df.index.droplevel(level=1)\n",
    "az_ipv_url_df.columns = ['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes using the index\n",
    "merged_df = pd.merge(az_ipv_data, az_ipv_url_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_ipv_merged_url_df = merged_df[['Text','City', 'Date', 'shooter_suicide', 'dv_history', 'Year', 'names', 'url']]\n",
    "az_ipv_merged_url_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather contents for each url related to the IPV incident\n",
    "- First check that the url is valid otherwise skip it\n",
    "- Grab the contents of the website using BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the url is valid\n",
    "def check_request_ok(url):\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        return r.status_code == requests.codes.ok\n",
    "    except:\n",
    "        print('Connection refused to {}'.format(url))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_check = az_ipv_merged_url_df.url.apply(check_request_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid urls\n",
    "az_ipv_merged_url_df = az_ipv_merged_url_df.loc[request_check].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text contents of each site. Skip some entries based on experience with some websites\n",
    "def get_url_text(url):\n",
    "    req = requests.get(url)\n",
    "    bs = BeautifulSoup(req.text)\n",
    "    ret_text = []\n",
    "    ret_text += [tag.text for tag in bs.findAll() if tag.name in ['p','text'] and len(tag.text.split()) > 10]\n",
    "    #print(ret_text)\n",
    "    #print(len(ret_text))\n",
    "    if ret_text and ret_text[0].find('compilation') < 0 :\n",
    "        return ret_text\n",
    "    else:\n",
    "        ret_text = []\n",
    "        #print('using entry-content')\n",
    "        for inst in bs.select('div[class*=\"entry-content\"]'):\n",
    "            ret_text.append(inst.get_text())\n",
    "        return [x for x in ret_text if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the text returned from each website to a new column\n",
    "az_ipv_merged_url_df['url_text'] = az_ipv_merged_url_df.url.apply(get_url_text)\n",
    "# and filter out entries that didn't return any text\n",
    "az_ipv_text_df = az_ipv_merged_url_df.loc[az_ipv_merged_url_df.url_text.apply(lambda x: len(x) > 0)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue filtering the entries to only the ones that appear to be related to the incident\n",
    "- Use keywords to see if the entry is valid\n",
    "- Also make sure the names of the individuals appear in the article\n",
    "- This reduces the number of articles down to the ones that are related to IPV\n",
    "- Also clean up the text for each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_list = ['kill', 'murder', 'arizona', 'victim', 'gun', 'homicide', 'arrest', 'manslaugter', 'police', 'argument']\n",
    "def check_for_keywords(row):\n",
    "    keyword_match = False\n",
    "    name_match = False\n",
    "    for cur_block in row.url_text:\n",
    "        #print(cur_block)\n",
    "        cur_block_lower = cur_block.lower()\n",
    "        name_list = [x.lower() for x in set(re.split(',| ',row.names))]\n",
    "        keyword_match = keyword_match | len([x for x in keywords_list if x in cur_block_lower]) > 0\n",
    "        name_match = name_match | len([x for x in name_list if x in cur_block_lower]) > 0\n",
    "        if keyword_match and name_match:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_check = az_ipv_text_df.apply(check_for_keywords, axis=1)\n",
    "az_ipv_news_df = az_ipv_text_df.loc[keyword_check].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some entries are in lists so combine them all in to a single string\n",
    "az_ipv_news_df['url_text'] = az_ipv_news_df.url_text.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the text since there are some tags in the strings\n",
    "def clean_text_data(text):\n",
    "    ret_string = ''\n",
    "    newline_regex = re.compile('\\\\n|\\\\xa0|\\\\t|<[a-z\"= ]+?>')\n",
    "    space_regex = re.compile('\\s\\s+')\n",
    "    ret_string = newline_regex.sub(' ', text)\n",
    "    ret_string = space_regex.sub(' ', ret_string)\n",
    "    return ret_string\n",
    "\n",
    "az_ipv_cleaned_text = az_ipv_news_df.copy()\n",
    "az_ipv_cleaned_text.loc[:, 'url_text'] = az_ipv_news_df.url_text.apply(clean_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classification model using text about each IPV incident\n",
    "- Built two models. One using the shooter suicide category as the response, \n",
    "- Another on the history of domestic violence as the response\n",
    "- First, since there are multiple articles for some incidents, decide on which on to use based on the amount of words and the number of IPV keywords that appear in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the responses as 0 or 1\n",
    "az_ipv_cleaned_text.loc[:, 'shooter_suicide'] = az_ipv_cleaned_text.shooter_suicide.apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "az_ipv_cleaned_text.loc[:, 'dv_history'] = az_ipv_cleaned_text.dv_history.apply(lambda x: 1 if x.strip() == 'Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_named_entities(text):\n",
    "    ret_list = []\n",
    "    # Reload en_core_web_sm model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    ret_list += [X.text for X in doc.ents if X.label_ == 'PERSON' and X.text.count(' ') == 0 and re.match('[A-Z]', X.text)]\n",
    "    return([x.lower() for x in set(ret_list) if len(x) > 3])\n",
    "\n",
    "az_ipv_cleaned_text['url_names'] = az_ipv_cleaned_text.url_text.apply(extract_named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['kill', 'murder', 'arizona', 'victim', 'gun', 'homicide', 'arrest', 'manslaugter', 'police', 'argument']\n",
    "def get_num_keywords(row):\n",
    "        #print(cur_block)\n",
    "        cur_keywords = keywords + [x.lower() for x in re.split(',| ', row.names)]\n",
    "        keyword_match = [x for x in cur_keywords if x in row.url_text]\n",
    "        return len(keyword_match)\n",
    "\n",
    "az_ipv_cleaned_text['url_text_length'] = az_ipv_cleaned_text.url_text.apply(lambda x: len(x))\n",
    "az_ipv_cleaned_text['keyword_count'] = az_ipv_cleaned_text.apply(get_num_keywords, axis=1)\n",
    "groupby_index = az_ipv_cleaned_text.reset_index().groupby('index').agg({'url_text_length':'max', 'keyword_count':'max'})\n",
    "groupby_index.rename({'url_text_length':'max_text_length', 'keyword_count':'max_keyword_count'}, axis=1, inplace=True)\n",
    "az_merged_df = pd.merge(az_ipv_cleaned_text, groupby_index, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = az_ipv_cleaned_text.url_names.sum()\n",
    "# Some last names happen to be common words so don't remove those\n",
    "names_to_remove = ['manslaughter', 'ar-15/m16', 'bias', 'bigamy', 'blanks', 'broken', 'case', 'child', 'colt', 'drive', \n",
    "                   'fill', 'girl', 'husband', 'jesus', 'long', 'neighbors', 'organization', 'rich', 'scene', 'sheriff',\n",
    "                   'sirens', 'smart', 'xanax', 'young', 'younger', 'burger', 'burritos', 'glass']\n",
    "name_list = [x for x in name_list if x not in names_to_remove]\n",
    "az_reduced_data = az_merged_df[['Text', 'City', 'Date', 'shooter_suicide', 'dv_history', 'prior_convict', 'order_of_protect', \n",
    "                             'require_turn_in_firearm', 'fed_prohib', 'Year', 'url','url_text', 'names', 'url_text_length', \n",
    "                             'keyword_count', 'max_text_length', 'max_keyword_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max_mask = az_reduced_data.keyword_count == az_reduced_data.max_keyword_count\n",
    "az_reduced_data = az_reduced_data.loc[az_reduced_data.apply(lambda x: x.url_text_length == x.max_text_length, axis=1)].copy()\n",
    "az_reduced_data.drop_duplicates(inplace=True)\n",
    "# Combine the text from the pdf with the text from the news articles to increase the words count associated with each article\n",
    "#az_reduced_data['combined_text'] = az_reduced_data.url_text + az_reduced_data.Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP analysis of text from new articles\n",
    "- Updating the stop word list based on some words that were appearing in the model, but did not give meaningful results\n",
    "- Also removing the list of names so that the model doesn't overfit the training data \n",
    "- Perform grid search cross validation to find the best settings for the models.\n",
    "- Use a pipeline to first create a TF-IDF vectorizer and then put that through a stochastic gradient descent classifier that used a linear SVM kernel\n",
    "\n",
    "### Domestic violence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_stopwords = ['dallas', 'anthony', 'turquoise', 'manuel', 'phoenix', 'tpd', 'phoenix', 'pima', 'mohave',\n",
    "                    'thompson', 'jr', 'ramapo', 'tucson', 'rios', 'county', 'trevino', 'mccormick', 'deckard', 'tia', \n",
    "                    'vicki', 'caldwell', 'luberda', 'scottsdale', 'havasu', 'azcentral', 'la', 'mccaskill', \n",
    "                    'wayne', 'douglas', 'jay', 'adtech_pagealias', 'chicago', 'rosarito', 'margaret', 'css', \n",
    "                    'ford', 'de', 'los', 'seattle', 'carolina', 'martos', 'morales', 'san', 'joe', 'torres', \n",
    "                    'se', 'perez', 'doris', 'west', 'gilbert', 'oro', 'ismael', 'bodine', 'martos', 'ulan', 'los',\n",
    "                    'flagstaff', 'torres', 'comments', 'var', 'readers', 'reading', 'nyland', 'las', 'unlimited', \n",
    "                    'philip', 'joshua', 'bali', 'lizzie', 'wakeham', 'sanders', 'dagle', 'tina', 'vondran', 'dewitteâ', \n",
    "                    'mesa', 'east', 'city', 'theresa', 'beaver', 'gallegos', 'scharge', 'pascual', 'ned', 'california']\n",
    "new_stopwords = set(stopwords.words('english') + name_list + manual_stopwords)\n",
    "\n",
    "dv_stopwords = ['domestic', 'violence'] + list(new_stopwords)\n",
    "dv_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=dv_stopwords, sublinear_tf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', n_jobs=2, random_state=42, class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to vary during grid search\n",
    "my_grid_dv = {'tfidf__ngram_range':((1,2), (1,3)), 'tfidf__min_df': (2, 5), 'tfidf__max_df': (0.9, 0.8), \n",
    "              'tfidf__use_idf': (True, False), 'tfidf__max_features': (10000, 25000, 50000), \n",
    "              'clf__alpha': (1e-3, 1e-5, 1e-7), 'clf__max_iter': (10, 50, 80)}\n",
    "grid_cv_dv = GridSearchCV(estimator=dv_pipeline, n_jobs=5, cv=5, param_grid=my_grid_dv, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the domestic \n",
    "X_train_dv, X_test_dv, Y_train_dv, Y_test_dv = train_test_split(az_reduced_data.url_text, \n",
    "                                                                az_reduced_data.dv_history, random_state=42, \n",
    "                                                                test_size=0.2, stratify=az_reduced_data.dv_history)\n",
    "grid_cv_dv.fit(X_train_dv, Y_train_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparamters found during grid search\n",
    "print(grid_cv_dv.best_params_)\n",
    "print(grid_cv_dv.best_score_)\n",
    "Y_test_predict_dv = grid_cv_dv.best_estimator_.predict(X_test_dv)\n",
    "print(grid_cv_dv.best_estimator_.score(X_test_dv, Y_test_dv))\n",
    "confusion_matrix(y_pred=Y_test_predict_dv, y_true=Y_test_dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv_pipeline.set_params(**grid_cv_dv.best_params_)\n",
    "dv_pipeline.fit(X_train_dv, Y_train_dv)\n",
    "print(dv_pipeline.score(X_test_dv, Y_test_dv))\n",
    "best_sdg_dv = dv_pipeline.named_steps.clf\n",
    "best_tfidf_dv = dv_pipeline.named_steps.tfidf\n",
    "X_train_tfidf_dv = best_tfidf_dv.transform(X_train_dv)\n",
    "coef_dict_dv = dict(zip(best_tfidf_dv.get_feature_names(), map(lambda x: x[0],best_sdg_dv.coef_.T.tolist())))\n",
    "sorted_coef_list_dv = sorted(coef_dict_dv.items(), key=lambda kv: abs(kv[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance based on the terms from the TF-IDF vectorizer\n",
    "sorted_coef_list_dv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shooter suicide model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ss, X_test_ss, Y_train_ss, Y_test_ss = train_test_split(az_reduced_data.url_text, \n",
    "                                                                az_reduced_data.shooter_suicide, \n",
    "                                                                random_state=42, test_size=0.2, \n",
    "                                                                stratify=az_reduced_data.shooter_suicide)\n",
    "ss_stopwords = list(new_stopwords) + ['suicide']\n",
    "ss_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=ss_stopwords, sublinear_tf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', n_jobs=2, random_state=42, class_weight='balanced')),\n",
    "])\n",
    "my_grid_ss = {'tfidf__ngram_range':((1,2), (1,3)), 'tfidf__min_df': (2, 5), 'tfidf__max_df': (0.9, 0.8), \n",
    "              'tfidf__use_idf': (True, False), 'tfidf__max_features': (10000, 25000, 50000), \n",
    "              'clf__alpha': (1e-3, 1e-5, 1e-7), 'clf__max_iter': (10, 50, 80)}\n",
    "grid_cv_ss = GridSearchCV(estimator=ss_pipeline, n_jobs=5, cv=5, param_grid=my_grid_ss, verbose=3)\n",
    "grid_cv_ss.fit(X_train_ss, Y_train_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_cv_ss.best_params_)\n",
    "print(grid_cv_ss.best_score_)\n",
    "Y_test_predict_ss = grid_cv_ss.best_estimator_.predict(X_test_ss)\n",
    "print(grid_cv_ss.best_estimator_.score(X_test_ss, Y_test_ss))\n",
    "confusion_matrix(y_pred=Y_test_predict_ss, y_true=Y_test_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importance\n",
    "ss_pipeline.set_params(**grid_cv_ss.best_params_)\n",
    "ss_pipeline.fit(X_train_ss, Y_train_ss)\n",
    "print(ss_pipeline.score(X_test_ss, Y_test_ss))\n",
    "best_sdg_ss = ss_pipeline.named_steps.clf\n",
    "best_tfidf_ss = ss_pipeline.named_steps.tfidf\n",
    "X_train_tfidf_ss = best_tfidf_ss.transform(X_train_ss)\n",
    "coef_dict_ss = dict(zip(best_tfidf_ss.get_feature_names(), map(lambda x: x[0],best_sdg_ss.coef_.T.tolist())))\n",
    "sorted_coef_list_ss = sorted(coef_dict_ss.items(), key=lambda kv: abs(kv[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_coef_list_ss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
